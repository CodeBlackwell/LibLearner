#!/usr/bin/env python3

import argparse
import os
import sys
import pandas as pd
import tempfile
import zipfile
import requests
from urllib.parse import urlparse
from collections import defaultdict
from pathlib import Path
from liblearner.file_processor import registry, DEFAULT_IGNORE_DIRS
from liblearner.processors import (
    PythonProcessor,
    JupyterProcessor,
    YAMLProcessor,
    MarkdownProcessor,
    JavaScriptProcessor,
    JSONProcessor,
    ShellProcessor
)

def is_github_url(url):
    """Check if a string is a GitHub URL."""
    try:
        parsed = urlparse(url)
        return parsed.netloc in ('github.com', 'www.github.com') and len(parsed.path.strip('/').split('/')) >= 2
    except:
        return False

def get_repo_info(url):
    """Extract owner and repo name from GitHub URL."""
    # Remove trailing .git if present
    url = url.rstrip('/')
    if url.endswith('.git'):
        url = url[:-4]
    
    parts = url.split('/')
    owner = parts[-2]
    repo = parts[-1]
    return owner, repo

def download_github_repo(url, target_dir):
    """Download and extract a GitHub repository zip file."""
    try:
        owner, repo = get_repo_info(url)
        print(f"Repository info - Owner: {owner}, Repo: {repo}")
        
        # Create target directory
        os.makedirs(target_dir, exist_ok=True)
        print(f"Created/verified directory:\n  Target: {target_dir}")
        
        # Construct the zip download URL
        zip_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/main.zip"
        print(f"Attempting download from: {zip_url}")
        
        # Download the zip file
        response = requests.get(zip_url, stream=True)
        print(f"Initial response status code: {response.status_code}")
        
        if response.status_code == 404:
            # Try master branch if main doesn't exist
            print("Main branch not found, trying master...")
            zip_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/master.zip"
            response = requests.get(zip_url, stream=True)
            print(f"Master branch response status code: {response.status_code}")
        
        response.raise_for_status()
        
        # Save zip file in the main resources directory
        zip_path = os.path.join(target_dir, f"{repo}.zip")
        print(f"Saving zip file to: {zip_path}")
        total_size = 0
        with open(zip_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    total_size += len(chunk)
                    f.write(chunk)
        print(f"Downloaded {total_size} bytes")
        
        if total_size == 0:
            raise Exception("Downloaded file is empty")
        
        if not os.path.exists(zip_path):
            raise Exception(f"Zip file was not created at {zip_path}")
        
        # Extract the zip file
        print(f"Extracting files to: {target_dir}")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            print(f"Zip contains {len(file_list)} files")
            zip_ref.extractall(target_dir)
        
        # Remove the zip file
        os.remove(zip_path)
        print("Removed temporary zip file")
        
        # Find the extracted directory (it might be named repo-main or repo-master)
        extracted_dirs = [d for d in os.listdir(target_dir) 
                        if os.path.isdir(os.path.join(target_dir, d)) and d.startswith(f"{repo}-")]
        print(f"Found extracted directories: {extracted_dirs}")
        
        if not extracted_dirs:
            raise Exception("Could not find extracted repository directory")
        
        # Rename the extracted directory to just the repo name
        old_path = os.path.join(target_dir, extracted_dirs[0])
        new_path = os.path.join(target_dir, repo)
        if os.path.exists(new_path):
            import shutil
            shutil.rmtree(new_path)
        os.rename(old_path, new_path)
        
        print(f"Repository extracted to: {new_path}")
        return new_path
        
    except requests.exceptions.RequestException as e:
        print(f"Error downloading repository: {str(e)}", file=sys.stderr)
        print(f"Response details: {e.response.text if hasattr(e, 'response') else 'No response details'}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"Unexpected error while downloading: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return None

def log_error(file_path, error_message):
    """Log errors to stderr."""
    print(f"Error processing {file_path}: {error_message}", file=sys.stderr)

def show_progress(file_path, file_type=None):
    """Display progress information."""
    type_info = f" ({file_type})" if file_type else ""
    print(f"Processing {file_path}{type_info}")

def get_file_extension(file_path):
    """Get the file extension from a path."""
    return os.path.splitext(file_path)[1][1:] or "no_extension"

def get_type_name(mime_type: str) -> str:
    """Convert MIME type to a safe filename component."""
    # Map common MIME types to simpler names
    type_map = {
        'text/x-python': 'python',
        'text/x-yaml': 'yaml',
        'application/x-yaml': 'yaml',
        'text/yaml': 'yaml',
        'application/yaml': 'yaml',
        'text/markdown': 'markdown',
        'text/javascript': 'javascript',
        'application/javascript': 'javascript',
        'text/x-shellscript': 'shell',
        'application/x-ipynb+json': 'notebook'
    }
    if mime_type in type_map:
        return type_map[mime_type]
    
    # For other types, convert to safe filename
    return mime_type.replace('/', '_').replace('+', '_').replace('-', '_')

def write_dataframes_to_csv(dataframes, output_dir, base_name="results"):
    """Write DataFrames to CSV files, both combined and separate.
    
    Args:
        dataframes: Dict mapping processor types to their DataFrames
        output_dir: Directory to write CSV files
        base_name: Base name for output files
    """
    # Create combined DataFrame
    combined_df = pd.concat(dataframes.values(), ignore_index=True)
    
    # Write combined results
    combined_path = os.path.join(output_dir, f"{base_name}_combined.csv")
    combined_df.to_csv(combined_path, index=False)
    
    # Write separate results for each type
    for proc_type, df in dataframes.items():
        if not df.empty:
            type_name = get_type_name(proc_type)
            type_path = os.path.join(output_dir, f"{base_name}_{type_name}.csv")
            # Add type column to identify source in separate files
            df_with_type = df.copy()
            df_with_type['processor_type'] = proc_type
            df_with_type.to_csv(type_path, index=False)
    
    return combined_path

def get_unique_output_dir(base_dir):
    """Generate a unique directory name by appending a numeric counter."""
    counter = 1
    unique_dir = base_dir
    while os.path.exists(unique_dir):
        unique_dir = f"{base_dir}_{counter}"
        counter += 1
    return unique_dir

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Process files of different types and extract information."
    )
    parser.add_argument(
        "input_paths",
        nargs='+',
        help="One or more paths/URLs to process. Can be files, directories, or GitHub repository URLs"
    )
    parser.add_argument(
        "-o", "--output",
        help="Output directory for results (default: ./resources)",
        default="./resources"
    )
    parser.add_argument(
        "--ignore-dirs",
        nargs="*",
        help=f"Additional directories to ignore (default ignored: {', '.join(sorted(DEFAULT_IGNORE_DIRS))})",
        default=[]
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Show detailed processing information"
    )
    parser.add_argument(
        "-vo", "--verbose-output",
        nargs="?",
        const="default",  # Used when -vo is specified without a value
        help="Write verbose output to log file (default: ./logs/process_TIMESTAMP.log)"
    )
    parser.add_argument(
        "--temp-dir",
        help="Directory to store downloaded repositories (default: ./resources)",
        default="./resources"
    )
    args = parser.parse_args()
    
    # Set default output directory if not specified
    if args.output is None:
        if is_github_url(args.input_paths[0]):
            _, repo = get_repo_info(args.input_paths[0])
            args.output = os.path.join(args.temp_dir, f"{repo}-main")
        else:
            target_name = os.path.basename(os.path.abspath(args.input_paths[0]))
            args.output = os.path.join(args.temp_dir, target_name)
    
    # Ensure the output directory is unique
    args.output = get_unique_output_dir(args.output)
    
    return args

def main():
    args = parse_arguments()
    
    # Create resources directory if it doesn't exist
    os.makedirs(args.temp_dir, exist_ok=True)
    
    # Process each input path
    for input_path in args.input_paths:
        if is_github_url(input_path):
            print(f"\nProcessing GitHub repository: {input_path}")
            repo_path = download_github_repo(input_path, args.temp_dir)
            if not repo_path:
                print(f"Failed to download repository: {input_path}", file=sys.stderr)
                continue
            print(f"Step 1: Downloading and extracting repository... {repo_path}")
            print("\nStep 2: Processing repository files...")
            process_path(repo_path, args)
        else:
            process_path(input_path, args)

def process_path(input_path, args):
    # Verify the input path exists before proceeding
    if not os.path.exists(input_path):
        print(f"Error: Path does not exist: {input_path}", file=sys.stderr)
        return
    
    # For GitHub repos, set output directory to resources/extracted/<repo>
    if os.path.dirname(input_path).endswith('resources'):
        repo_name = os.path.basename(input_path)
        output_dir = os.path.join(args.temp_dir, 'extracted', repo_name)
    else:
        output_dir = args.output
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Set up logging
    if args.verbose_output is not None:
        # Generate default log path if needed
        if args.verbose_output == "default":
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_dir = os.path.join(output_dir, "logs")
            args.verbose_output = os.path.join(log_dir, f"process_{timestamp}.log")
        
        # Create log directory if needed
        log_dir = os.path.dirname(args.verbose_output)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)
        
        registry.set_verbose(True, args.verbose_output)
        print(f"Verbose output will be written to: {args.verbose_output}")
    else:
        registry.set_verbose(args.verbose)
    
    # Set output directory for registry
    registry.set_output_dir(output_dir)
    
    # Register available processors
    registry.register_processor(PythonProcessor())
    # Re Architecting Processors 
    # registry.register_processor(ShellProcessor())
    # registry.register_processor(JupyterProcessor())
    # registry.register_processor(YAMLProcessor())
    # registry.register_processor(MarkdownProcessor())
    # registry.register_processor(JavaScriptProcessor())
    # registry.register_processor(JSONProcessor())
    
    # Process input path
    if os.path.isdir(input_path):
        if args.verbose:
            print(f"Processing directory: {input_path}")
            
        results = registry.process_directory(input_path, args.ignore_dirs)
        
        # Collect all DataFrames from processors
        processor_dfs = defaultdict(list)
        for processor in registry.get_unique_processors():
            if not processor.results_df.empty:
                proc_type = processor.get_supported_types()[0]  # Use first type as identifier
                processor_dfs[proc_type].append(processor.results_df)
        
        # Combine DataFrames for each processor type
        combined_dfs = {}
        for proc_type, dfs in processor_dfs.items():
            if dfs:
                combined_dfs[proc_type] = pd.concat(dfs, ignore_index=True)
        
        # Write results if we have any DataFrames
        if combined_dfs:
            base_name = os.path.basename(os.path.abspath(input_path))
            output_path = write_dataframes_to_csv(combined_dfs, output_dir, base_name)
            if args.verbose:
                print(f"Results written to: {output_path}")
                
        if args.verbose:
            print(f"\nProcessed directory '{input_path}':")
            for folder, folder_results in results.items():
                for result in folder_results:
                    if isinstance(result, dict):
                        show_progress(result['path'], result.get('type'))
                        if 'error' in result:
                            log_error(result['path'], result['error'])
    
    else:
        if args.verbose:
            print(f"Processing file: {input_path}")
        
        # Process single file
        failed_files = []  # List to keep track of failed files
        result = registry.process_file(input_path)
        
        if result:
            if isinstance(result, dict):
                if args.verbose:
                    show_progress(result['path'], result.get('type'))
                    if 'error' in result:
                        log_error(result['path'], result['error'])
                        failed_files.append(result['path'])  # Add to failed files list
                
                # Collect DataFrames from processors
                processor_dfs = {}
                for processor in registry._processors.values():
                    if not processor.results_df.empty:
                        proc_type = processor.get_supported_types()[0]  # Use first type as identifier
                        processor_dfs[proc_type] = processor.results_df
                
                # Write results if we have any DataFrames
                if processor_dfs:
                    base_name = os.path.splitext(os.path.basename(input_path))[0]
                    output_path = write_dataframes_to_csv(processor_dfs, output_dir, base_name)
                    if args.verbose:
                        print(f"Results written to: {output_path}")
            
            elif isinstance(result, str):
                if args.verbose:
                    print(f"Results written to: {result}")
        else:
            print(f"No processor available for file: {input_path}")
        
        # Report failed files
        if failed_files:
            print("\nFailed to process the following files:")
            for file in failed_files:
                print(f"- {file}")

if __name__ == "__main__":
    main()
