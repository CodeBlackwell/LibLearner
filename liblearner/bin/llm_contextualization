#!/usr/bin/env python3

import argparse
import os
import sys
import pandas as pd
import json
from openai import OpenAI
from pathlib import Path
from typing import Dict, List, Optional, Iterator
from datetime import datetime
from tqdm import tqdm
import colorama
from colorama import Fore, Style
import time

# Initialize colorama for cross-platform color support
colorama.init()

def load_csv_data(file_path: str) -> pd.DataFrame:
    """Load and validate CSV data from process_files output."""
    try:
        df = pd.read_csv(file_path)
        required_columns = ['filepath', 'content', 'processor_type']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")
            
        # Clean filepaths by removing everything up to and including 'resources/'
        df['filepath'] = df['filepath'].apply(lambda x: x.split('resources/')[-1] if 'resources/' in x else x)
        
        return df
    except Exception as e:
        print(f"Error loading CSV file: {str(e)}", file=sys.stderr)
        sys.exit(1)

def get_file_chunks(df: pd.DataFrame, limit: Optional[int] = None) -> Iterator[tuple[str, pd.DataFrame]]:
    """Generate chunks of data grouped by filepath.
    
    Args:
        df: DataFrame containing the data
        limit: Optional limit on number of files to process
    """
    grouped = df.groupby('filepath')
    if limit is not None:
        # Get list of unique filepaths and limit it
        filepaths = list(grouped.groups.keys())[:limit]
        # Only yield the limited groups
        for filepath in filepaths:
            yield filepath, grouped.get_group(filepath)
    else:
        yield from grouped

def prepare_individual_conversation(filepath: str, row: Dict, system_prompt: Optional[str] = None) -> List[Dict]:
    """
    Prepare conversation context for a single row.
    """
    messages = []
    
    # Add system prompt if provided
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    message = {
        "role": "user",
        "content": f"Analyzing row from file: {filepath}\n\nContent:\n{row['content']}"
    }
    messages.append(message)
    
    return messages

def calculate_token_cost(input_tokens: int, output_tokens: int) -> float:
    """
    Calculate the cost based on input and output tokens.
    """
    input_cost = (input_tokens / 1_000_000) * 2.50
    output_cost = (output_tokens / 1_000_000) * 10.00
    return input_cost + output_cost

def call_openai_with_retry(client: OpenAI, messages: List[Dict], model: str, temperature: float, retries: int = 3) -> Dict:
    """
    Call OpenAI API with retry logic to handle rate limit issues.
    Returns the response and token usage information.
    """
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature
            )
            return {
                "content": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens
                }
            }
        except Exception as e:
            wait_time = 2 ** attempt
            print(f"Attempt {attempt+1} failed: {str(e)}. Retrying in {wait_time} seconds...", file=sys.stderr)
            time.sleep(wait_time)
    print("All attempts failed. Exiting.", file=sys.stderr)
    sys.exit(1)

def call_openai_individual_rows(client: OpenAI, chunk_df: pd.DataFrame, model: str, temperature: float, system_prompt: Optional[str]) -> pd.DataFrame:
    """
    Conduct a conversation for each row individually and update the DataFrame with contextual information.
    """
    # Create an explicit copy of the DataFrame and reset the index
    working_df = chunk_df.copy().reset_index(drop=True)
    total_rows = len(working_df)
    current_file = working_df['filepath'].iloc[0]
    print(f"\nStarting individual row processing for {total_rows} rows from file: {current_file}")
    
    # Track token usage
    total_input_tokens = 0
    total_output_tokens = 0
    
    # Process each row individually
    for index, row in working_df.iterrows():
        messages = prepare_individual_conversation(current_file, row, system_prompt)
        
        # Call OpenAI API with retry logic for each row
        response = call_openai_with_retry(client, messages, model, temperature)
        response_text = response["content"]
        token_usage = response["usage"]
        
        # Update token usage totals
        total_input_tokens += token_usage["prompt_tokens"]
        total_output_tokens += token_usage["completion_tokens"]
        
        # Calculate current cost
        current_cost = calculate_token_cost(total_input_tokens, total_output_tokens)
        
        # Print token usage and cost in real-time
        print(f"Processed row {index + 1}/{total_rows} from file: {current_file}")
        print(f"Total input tokens used: {total_input_tokens}")
        print(f"Total output tokens used: {total_output_tokens}")
        print(f"Estimated cost so far: ${current_cost:.4f}")
        
        # Add the generated context to the row in the DataFrame
        working_df.at[index, 'context'] = response_text
    
    print(f"\nFinished processing file: {current_file}")
    return working_df

def process_file_with_conversation(client: OpenAI, filepath: str, chunk_df: pd.DataFrame, args) -> pd.DataFrame:
    """
    Handle conversation flow for a single file and save the results.
    """
    print(f"\n{Fore.CYAN}üìÑ Starting conversation for file: {filepath}{Style.RESET_ALL}")
    
    # Conduct conversation for each row individually
    updated_chunk_df = call_openai_individual_rows(client, chunk_df, args.model, args.temperature, args.system_prompt)
    
    return updated_chunk_df

def get_default_output_dir(input_file: str) -> str:
    """Generate default output directory path based on input file name."""
    # Get the base directory of the library
    lib_dir = Path(__file__).resolve().parent.parent
    
    # Create path to llm_output directory
    llm_output_dir = lib_dir / 'resources' / 'llm_output'
    
    # Get target name from input file
    target_name = Path(input_file).stem
    
    # Create full output path
    output_dir = llm_output_dir / target_name
    
    return str(output_dir)

def save_responses(df: pd.DataFrame, output_dir: str):
    """Save responses to a CSV file."""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save DataFrame with context column
        output_path = Path(output_dir) / f"processed_with_context_{timestamp}.csv"
        df.to_csv(output_path, index=False)
        
    except Exception as e:
        print(f"Error saving responses: {str(e)}", file=sys.stderr)
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(
        description="Process CSV output from process_files and send to OpenAI API"
    )
    parser.add_argument(
        "input_file",
        help="CSV file containing process_files output"
    )
    parser.add_argument(
        "-o", "--output-dir",
        help="Output directory for API responses (default: liblearner/resources/llm_output/<target_name>)",
        default=None
    )
    parser.add_argument(
        "--model",
        help="OpenAI model to use (default: gpt-4o)",
        default="gpt-4o"
    )
    parser.add_argument(
        "--temperature",
        help="Temperature for response generation (default: 0.1)",
        type=float,
        default=0.1
    )
    parser.add_argument(
        "--system-prompt",
        help="Optional system prompt to guide the model",
        default="Provide a summary of the content of the file."
    )
    parser.add_argument(
        "--api-key",
        help="OpenAI API key (defaults to OPENAI_API_KEY environment variable)",
        default=os.environ.get('OPENAI_API_KEY')
    )
    parser.add_argument(
        "-n", "--num-files",
        help="Limit the number of files to process",
        type=int,
        default=5
    )
    
    args = parser.parse_args()
    
    # Set default output directory if not specified
    if args.output_dir is None:
        args.output_dir = get_default_output_dir(args.input_file)
    
    # Create output directory structure
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Check for API key
    if not args.api_key:
        print("Error: OpenAI API key not found. Either use --api-key or set OPENAI_API_KEY environment variable.",
              file=sys.stderr)
        sys.exit(1)
    
    client = OpenAI(api_key=args.api_key)
    
    # Load and process data
    df = load_csv_data(args.input_file)
    responses = []
    
    # Get total number of files and create file iterator
    total_files = len(df['filepath'].unique())
    limit_msg = f" (limited to {args.num_files})" if args.num_files else ""
    print(f"\n{Fore.BLUE}üîç Found {total_files} unique files{limit_msg}{Style.RESET_ALL}")
    
    file_chunks = list(get_file_chunks(df, args.num_files))
    
    try:
        # Process each file chunk with progress bar
        for filepath, chunk_df in tqdm(file_chunks, desc="Processing files", unit="file"):
            updated_chunk_df = process_file_with_conversation(client, filepath, chunk_df, args)
            responses.append(updated_chunk_df)
        
        # Concatenate all updated DataFrames
        final_df = pd.concat(responses, ignore_index=True)
        
        # Save all responses
        save_responses(final_df, args.output_dir)
        print(f"\n{Fore.GREEN}‚úÖ Successfully processed all files!{Style.RESET_ALL}")
        print(f"{Fore.BLUE}üìÅ Responses saved to: {args.output_dir}{Style.RESET_ALL}")
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è  Processing interrupted by user{Style.RESET_ALL}")
        if responses:
            print(f"{Fore.BLUE}üíæ Saving partial results...{Style.RESET_ALL}")
            final_df = pd.concat(responses, ignore_index=True)
            save_responses(final_df, args.output_dir)
            print(f"{Fore.GREEN}‚úÖ Partial results saved to: {args.output_dir}{Style.RESET_ALL}")
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Error during processing: {str(e)}{Style.RESET_ALL}", file=sys.stderr)
        if responses:
            print(f"{Fore.BLUE}üíæ Attempting to save partial results...{Style.RESET_ALL}")
            final_df = pd.concat(responses, ignore_index=True)
            save_responses(final_df, args.output_dir)
            print(f"{Fore.GREEN}‚úÖ Partial results saved to: {args.output_dir}{Style.RESET_ALL}")
        sys.exit(1)

if __name__ == "__main__":
    main()
